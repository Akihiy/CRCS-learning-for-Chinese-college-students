{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 张量（Tensor）是MindSpore网络运算中的基本数据结构。张量中的数据类型可参考dtype。\n",
    "# 不同维度的张量分别表示不同的数据，0维张量表示标量，1维张量表示向量，2维张量表示矩阵，3维张量可以表示彩色图像的RGB三通道等等。\n",
    "# MindSpore张量支持不同的数据类型，包含int8、int16、int32、int64、uint8、uint16、uint32、uint64、float16、float32、float64、bool_，与NumPy的数据类型一一对应。\n",
    "# 在MindSpore的运算处理流程中，Python中的int数会被转换为定义的int64类型，float数会被转换为定义的float32类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# 导入 MindSpore\n",
    "import mindspore\n",
    "from mindspore import dtype \n",
    "from mindspore import Tensor\n",
    "# cell 同时输出多行\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#指定数据类型\n",
    "a=1\n",
    "type(a)\n",
    "b = Tensor(a, dtype.float64)\n",
    "b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造张量时，支持传入Tensor、float、int、bool、tuple、list和NumPy.array类型，其中tuple和list里只能存放float、int、bool类型数据。\n",
    "# Tensor初始化时，可指定dtype。如果没有指定dtype，初始值int、float、bool分别生成数据类型为mindspore.int32、mindspore.float32、mindspore.bool_的0维Tensor， \n",
    "# 初始值tuple和list生成的1维Tensor数据类型与tuple和list里存放的数据类型相对应，如果包含多种不同类型的数据，则按照优先级：bool < int < float，\n",
    "# 选择相对优先级最高类型所对应的mindspore数据类型。\n",
    "# 如果初始值是Tensor，则生成的Tensor数据类型与其一致；如果初始值是NumPy.array，则生成的Tensor数据类型与之对应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as npfrom mindspore import Tensor\n",
    "#用数组创建张量\n",
    "x = Tensor(np.array([[1, 2], [3, 4]]), dtype.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用数值创建张量 \n",
    "y = Tensor(1.0, dtype.int32) \n",
    "z = Tensor(2, dtype.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用Bool创建张量 \n",
    "m = Tensor(True, dtype.bool_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用tuple创建张量 \n",
    "n = Tensor((1, 2, 3), dtype.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用list创建张量\n",
    "p = Tensor([4.0, 5.0, 6.0], dtype.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用常量创建张量 \n",
    "q = Tensor(1, dtype.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 张量的属性包括形状 (shape) 和数据类型 (dtype)\n",
    "# 形状: Tensor的 shape，是一个 tuple。数据类型: Tensor的 dtype，是 MindSpore 的一个数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Tensor(np.array([[1, 2], [3, 4]]), dtype.int32) \n",
    "x.shape # 形状 x.dtype # 数据类型 x.ndim # 维度 x.size # 大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 张量的方法\n",
    "# asnumpy():将 Tensor 转换为 NumPy的 array。\n",
    "y = Tensor(np.array([[True, True], [False, False]]), dtype.bool_) \n",
    "# 将Tensor数据类型转换成NumPy \n",
    "y_array = y.asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集介绍:\n",
    "# MNIST 数据集来自美国国家标准与技术研究所，National lnstitute of standards andTechnology(NIST),\n",
    "# 数据集由来自 250 个不同人手写的数字构成，其中 50%是高中学生50%来自人门普查局 (the Census Bureau) 的工作人员\n",
    "# 训练集: 60000，测试集: 10000\n",
    "# 下载MNIST数据集：https://zhuanyejianshe.obs.cn-north-4.myhuaweicloud.com/chuangxinshijianke/cv-nlp/MNIST.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import mindspore.dataset as ds # 数据集的载入 \n",
    "import matplotlib.pyplot as plt\n",
    "dataset_dir = \"./MNIST/train\" # 数据集路径 #\n",
    "从mnist dataset读取3张图片 mnist_dataset = ds.MnistDataset(dataset_dir=dataset_dir, num_samples=3) \n",
    "# 设置图像大小 \n",
    "plt.figure(figsize=(8,8)) i = 1 \n",
    "# 打印3张子图 \n",
    "for dic in mnist_dataset.create_dict_iterator(output_numpy=True): \n",
    "    plt.subplot(3,3,i) \n",
    "    plt.imshow(dic['image'][:,:,0]) \n",
    "    plt.axis('off') \n",
    "    i +=1 \n",
    "plt.show()\n",
    "# MindSpore还支持加载多种数据存储格式下的数据集，用户可以直接使用mindspore.dataset中对应的类加载磁盘中的数据文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理在众多深度学习算法中都起着重要作用，数据预处理可以提高模型精度，加快模型收敛速度，提升模型性能，\n",
    "# 这里主要介绍 MindSpore 常用的数据预处理方式。数据是深度学习的基础，良好的数据输入可以对整个深度神经网络训练起到非常积极的作用。\n",
    "# 在训练前对已加载的数据集进行数据处理，可以解决诸如数据量过大、样本分布不均等问题从而获得更加优化的数据输入。\n",
    "# 定义数据预处理函数，函数功能包括\n",
    "# 加载MNIST 数据集\n",
    "# 读取数据集 MnistDataset(data path)\n",
    "# 打乱数据集 shuffle\n",
    "# 对数据集进行混洗，随机打乱数据顺序。设定的 buffer size 越大，混洗程度越大，但时间、计算资源消耗也会更大\n",
    "# 专换图像通道 HWC2CHW\n",
    "# 寄存储的图像由原来的(高，宽，通道数)转换为(通道数，高，宽).比量输出数据 Batch\n",
    "# 寄数据集分批，分别输入到训练系统中进行训练，可以减少训练轮次，达到加速训练过程的目的。\n",
    "# 重复 Repeat\n",
    "# 对数据集进行重复，达到扩充数据量的目的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.dataset.transforms.c_transforms as C # 常用转化算子 \n",
    "import mindspore.dataset.vision.c_transforms as CV # 图像转化算子 \n",
    "from mindspore.common import dtype as mstype # 数据形态转换\n",
    "from mindspore.common.initializer import Normal # 参数初始化 \n",
    "def create_dataset(data_path, batch_size=32): \n",
    "    \"\"\" 数据预处理与批量输出的函数 Args: data_path: 数据路径 batch_size: 批量大小\n",
    "    \"\"\"\n",
    "    # 加载数据集 \n",
    "    data = ds.MnistDataset(data_path) \n",
    "    # 打乱数据集 \n",
    "    data = data.shuffle(buffer_size=10000) \n",
    "    # 数据标准化参数 # MNIST数据集的 mean = 33.3285，std = 78.5655\n",
    "    mean, std = 33.3285, 78.5655 \n",
    "    # 定义算子 \n",
    "    nml_op = lambda x : np.float32((x-mean)/std) \n",
    "    # 数据标准化，image = (image-mean)/std \n",
    "    hwc2chw_op = CV.HWC2CHW() # 通道前移（为配适网络，CHW的格式可最佳发挥昇腾芯片算力） \n",
    "    type_cast_op = C.TypeCast(mstype.int32) # 原始数据的标签是unint，计算损失需要int\n",
    "    # 算子运算 \n",
    "    data = data.map(operations=type_cast_op, input_columns='label') \n",
    "    data = data.map(operations=nml_op, input_columns='image') \n",
    "    data = data.map(operations=hwc2chw_op, input_columns='image')\n",
    "    # 批处理 \n",
    "    data = data.batch(batch_size) # 重复 \n",
    "    data = data.repeat(1) \n",
    "    return data\n",
    "train_path = os.path.join('MNIST','train') # 训练集路径 \n",
    "train_data = create_dataset(train_path) # 定义训练数据集 \n",
    "test_path = os.path.join('MNIST','test') # 测试集路径 \n",
    "test_data = create_dataset(test_path) # 定义测试数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤1全连接神经网络\n",
    "# 全连接层\n",
    "# mindspore.nn.Dense\n",
    "# in_channels: 输入通道\n",
    "# out channels: 输出通道\n",
    "# weight init: 权重初始化，Default 'normal'\n",
    "import mindspore.nn as nn \n",
    "from mindspore import Tensor \n",
    "# 构造输入张量 \n",
    "input = Tensor(np.array([[1, 1, 1], [2, 2, 2]]), mindspore.float32) \n",
    "print(input) \n",
    "# 构造全连接网络，输入通道为3，输出通道为3 \n",
    "net = nn.Dense(in_channels=3, out_channels=3, weight_init=1) \n",
    "output = net(input) \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤2 卷积神经网络\n",
    "# 卷积层\n",
    "# mindspore.nn.Conv2d\n",
    "# in channels: 输入通道\n",
    "# out channels: 输出通道\n",
    "# kernel_size : 核大小\n",
    "# stride : 步长\n",
    "# pad mode : padding 方式 (“same\", “valid”,“pad”) . Default: “same”.\n",
    "# Padding: padding 补产数字\n",
    "# has bias: 是否有偏置项 Default: False.\n",
    "# weight init: 权重初始化 Default: normal.\n",
    "# bias init: 偏置项初始化 Default:'zeros'\n",
    "# data_format：数据形状（ ‘NHWC’ or ‘NCHW’ ）. Default: ‘NCHW’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore \n",
    "import mindspore.nn as nn \n",
    "from mindspore import Tensor\n",
    "# mindspore.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, pad_mode='same', padding=0, dilation=1,\n",
    "#                         group=1, has_bias=False, weight_init='normal', bias_init='zeros', data_format='NCHW')\n",
    "# 图片数，通道数，图像高，图像宽 input = Tensor(np.ones([1, 3, 1080, 960]), mindspore.float32) \n",
    "# 输入通道数为3，输出通道数为24， 卷积核大小为5， 步长为1，padding方式same，有偏置项，权重初始化为normal \n",
    "net = nn.Conv2d(in_channels=3, out_channels=24, kernel_size=5, stride=1, pad_mode='same', has_bias=True, weight_init='normal')\n",
    "# 图片数，通道数，图像高，图像宽 \n",
    "output = net(input).shape \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤3矫正线性单元激活函数\n",
    "# mindspore.nn.ReLU\n",
    "input_x = Tensor(np.array([-1, 2, -3, 2, -1]), mindspore.float16) \n",
    "relu = nn.ReLU() \n",
    "output = relu(input_x) \n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤4 池化层\n",
    "# 2 维数据最大池化\n",
    "# mindspore.nn.MaxPool2d\n",
    "# kernel size : 池化大小 Default: 1.\n",
    "# Stride : 池化步长 Default: 1.\n",
    "# pad mode: padding 万式 (“same” or “valid”) . Default: “valid”.\n",
    "# data format: 数据形式 (NHWc'or NCHW’) .Default: NCHW\n",
    "input_x = np.random.randint(0, 10, [1, 2, 4, 4]) \n",
    "print(input_x) \n",
    "print(\"-----------------------------------\")\n",
    "# 最大池化，池化大小2x2， 步长为2 \n",
    "pool = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "output = pool(Tensor(input_x, mindspore.float32)) \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤 5 Flatten 层\n",
    "# mindspore.nn.Flatten\n",
    "# Flatten 层可以把多维的输入一维化，常用在卷积层到全连接层的过渡。\n",
    "from mindspore import Tensor \n",
    "import mindspore \n",
    "import mindspore.nn as nn \n",
    "input = Tensor(np.array([[[1, 1], [2, 2]], [[3, 3], [4, 4]]]), mindspore.float32) \n",
    "print(input) \n",
    "print(\"-----------------------------------\") \n",
    "net = nn.Flatten() \n",
    "output = net(input) \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤6搭建模型 (LeNet-5)\n",
    "# 所有神经网络的基类\n",
    "# mindspore.nn.Cell\n",
    "# 1. INPUT (输入层): 输入 28*28 的图片\n",
    "# c1 (卷积层): 选取 6个 5*5 卷积核(不包含偏置)，得到 6 个特征图，每个特征图的一个2.\n",
    "# 边为 28-5+1=24。\n",
    "# s2 (池化层) : 池化层是一个下采样层，输出 12*12*6 的特征图3选取 16 个大小为 5*5 积核，得到特征图大小为 8*8*16C3 (卷积层) :S4 (池化层) : 窗口大小为 2*2，输出 4*4*16 的特征图。\n",
    "# F5 (全连接层) : 120 个神经元\n",
    "# F6 (全连接层) : 84 个神经元\n",
    "# OUTPUT (输出层) : 10 个神经元，10 分类问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn # 各类网络层都在 nn 里面\n",
    "class LeNet5(nn.Cell):\n",
    "# 定义算子\n",
    "    def__init_self, num_class=10, num_channel=1):\n",
    "        super(LeNet5, self)._init__()\n",
    "        # 卷积层\n",
    "        self.conv1 = nn.Conv2d(num channel, 6, 5, pad mode='valid')self.conv2 = nn.Conv2d(6, 16, 5, pad mode='valid')\n",
    "        #全连接层\n",
    "        self.fc1 = nn.Dense(4 * 4 * 16, 120, weight init=Normal(0.02))\n",
    "        self.fc2 = nn.Dense(120, 84, weight init=Normal(0.02))\n",
    "        self.fc3 = nn.Dense(84, num class, weight init=Normal(0.02))\n",
    "        # 活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        #最大池化成\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel size=2, stride=2)\n",
    "        #网络展开\n",
    "        self.flatten = nn.Flatten()\n",
    "    # 建构网络\n",
    "    def construct(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x) \n",
    "        x = self.fc2(x) \n",
    "        x = self.relu(x) \n",
    "        x = self.fc3(x) \n",
    "        return x\n",
    "# 定义神经网络 \n",
    "lenet = LeNet5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练与评估\n",
    "# 步骤 1 损失函数\n",
    "# 交叉损失函数，用于分类模型。当标签数据不是 one-hot 编码形式时，:需要输入参数 sparse为 True。\n",
    "# mindspore.nn.SoftmaxCrossEntropyWithLogits代码:\n",
    "#定义交叉损失函数\n",
    "net loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)\n",
    "\n",
    "\n",
    "# 步骤2 优化器\n",
    "# 深度学习优化算法大概常用的有 SGD、Adam、Ftrl、lazyadam、Momentum、RMSprop、LarsProximal ada grad 和 lamb 这几种\n",
    "# 动量优化器\n",
    "# mindspore.nn.Momentum\n",
    "# 代码:\n",
    "# 定义优化器\n",
    "Ir = 0.01\n",
    "momentum = 0.9\n",
    "net opt = nn.Momentum(lenet.trainable params(), Ir, momentum)\n",
    "\n",
    "# 步骤3 模型编译\n",
    "# mindspore.Modelnetwork: 神经网络loss fn: 损失函数optimizer: 优化器metrics: 评估指标\n",
    "from mindspore import Model \n",
    "# 承载网络结构 \n",
    "from mindspore.nn.metrics import Accuracy \n",
    "# 测试模型用 \n",
    "# 模型编译 \n",
    "model = Model(lenet, net_loss, net_opt, metrics={'accuracy': Accuracy()})\n",
    "\n",
    "# 步骤4模型训练\n",
    "# model.train\n",
    "# epoch: 训练次数train dataset : 训练集\n",
    "#设定loss监控 f\n",
    "rom mindspore.train.callback import LossMonitor \n",
    "loss_cb = LossMonitor(per_print_times=train_data.get_dataset_size()) \n",
    "# 训练模型 \n",
    "model.train(3, train_data, loss_cb) # 训练3个epoch\n",
    "\n",
    "# 步骤 5 模型评估\n",
    "# model.eval\n",
    "# 评估模型 \n",
    "model.eval(test_data) # 测试网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本实验介绍了MindSpore的数据结构与类型，以及MindSpore搭建神经网络用到的基础模块，\n",
    "# 让学员学会如何加载数据集，搭建神经网络，训练和评估模型等，\n",
    "# 从易到难，由浅入深，让学员熟悉MindSpore的基础用法，掌握MindSpore开发的简单流程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
