{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './input/'\n",
    "\n",
    "train_columns = ['ip', 'app', 'device', 'os', 'channel', 'click_time', 'is_attributed']\n",
    "test_columns  = ['ip', 'app', 'device', 'os', 'channel', 'click_time', 'click_id']\n",
    "dtypes = {\n",
    "        'ip'            : 'uint32',\n",
    "        'app'           : 'uint16',\n",
    "        'device'        : 'uint16',\n",
    "        'os'            : 'uint16',\n",
    "        'channel'       : 'uint16',\n",
    "        'is_attributed' : 'uint8',\n",
    "        'click_id'      : 'uint32'\n",
    "        }\n",
    "\n",
    "# 97903891 = 184903891 - 87000000\n",
    "train = pd.read_csv(path+'train.csv', usecols=train_columns, dtype=dtypes, skiprows=range(1, 97903891), nrows=87000000, parse_dates=['click_time'])\n",
    "test = pd.read_csv(path+'test.csv', usecols=test_columns, dtype=dtypes, parse_dates=['click_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 训练集label\n",
    "y_train = train['is_attributed'].values\n",
    "\n",
    "sub = test[['click_id']]\n",
    "del test['click_id']\n",
    "\n",
    "# 训练集与测试集合并\n",
    "data = pd.concat([train, test], axis=0)\n",
    "del train, test\n",
    "gc.collect()\n",
    "\n",
    "# 时间处理\n",
    "data['day'] = data['click_time'].dt.day.astype('uint8')\n",
    "data['hour'] = data['click_time'].dt.hour.astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 统计特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:48<00:00, 14.06s/it]\n",
      "100%|██████████| 4/4 [03:50<00:00, 60.26s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count\n",
    "for cols in tqdm([['ip'],['ip','os','device'],['ip','day','hour']]):\n",
    "    name = '_'.join(cols)\n",
    "    data[name+'_cnts'] = data.groupby(cols)['click_time'].transform('count')\n",
    "    data[name+'_cnts'] = data[name+'_cnts'].astype('uint16')\n",
    "\n",
    "# nunique\n",
    "for f1 in ['ip']:\n",
    "    for f2 in tqdm(['app','device','os','channel']):\n",
    "        data[f1+'_'+f2+'_nuni'] = data.groupby([f1])[f2].transform('nunique')\n",
    "        data[f1+'_'+f2+'_nuni'] = data[f1+'_'+f2+'_nuni'].astype('uint16') \n",
    "            \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 时间差特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [22:58<00:00, 660.81s/it]\n"
     ]
    }
   ],
   "source": [
    "for cols in tqdm([['ip','os','device','app'],['ip','os','device','app','day']]):\n",
    "    for i in range(1,6):\n",
    "        \n",
    "        data['ct'] = (data['click_time'].astype(np.int64)//10**9).astype(np.int32)\n",
    "        \n",
    "        name = '{}_next_{}_click'.format('_'.join(cols), str(i))\n",
    "        data[name] = (data.groupby(cols).ct.shift(-i)-data.ct).astype(np.float32)\n",
    "        data[name] = data[name].fillna(data[name].mean())\n",
    "        data[name] = data[name].astype('uint16')\n",
    "        \n",
    "        name = '{}_lag_{}_click'.format('_'.join(cols), str(i))\n",
    "        data[name] = (data.groupby(cols).ct.shift(i)-data.ct).astype(np.float32)\n",
    "        data[name] = data[name].fillna(data[name].mean())\n",
    "        data[name] = data[name].astype('uint16')\n",
    "        \n",
    "        data.drop(['ct'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = ['ip', 'os', 'device', 'app']\n",
    "data['click_user_lab'] = 0\n",
    "pos = data.duplicated(subset=subset, keep=False)\n",
    "data.loc[pos, 'click_user_lab'] = 1\n",
    "pos = (~data.duplicated(subset=subset, keep='first')) & data.duplicated(subset=subset, keep=False)\n",
    "data.loc[pos, 'click_user_lab'] = 2\n",
    "pos = (~data.duplicated(subset=subset, keep='last')) & data.duplicated(subset=subset, keep=False)\n",
    "data.loc[pos, 'click_user_lab'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 排序特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [06:00<00:00, 175.75s/it]\n"
     ]
    }
   ],
   "source": [
    "for cols in tqdm([['ip','os','device','app'],['ip','os','device','app','day']]):\n",
    "    name = '{}_click_asc_rank'.format('_'.join(cols)) \n",
    "    data[name] = data.groupby(cols)['click_time'].rank(ascending=True)\n",
    "    \n",
    "    name = '{}_click_dec_rank'.format('_'.join(cols)) \n",
    "    data[name] = data.groupby(cols)['click_time'].rank(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练集/验证集/测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['ip','app','os','channel','device','day','hour']\n",
    "features = [f for f in data.columns if f not in ['click_time','is_attributed']]\n",
    "numerical_features = [f for f in features if f not in categorical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sparse_feats(data, cols):\n",
    "    for f in cols:\n",
    "        data[f] = data[f].fillna(-999)\n",
    "        data[f] = data[f].map(dict(zip(data[f].unique(), range(0, data[f].nunique()))))\n",
    "    return data\n",
    "\n",
    "data = process_sparse_feats(data, categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_x = data[:82000000][features]\n",
    "val_x = data[82000000:87000000][features]\n",
    "trn_y = y_train[:82000000]\n",
    "val_y = y_train[82000000:87000000]\n",
    "\n",
    "test_x = data[87000000:][features]\n",
    "\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepfm_model(sparse_columns, dense_columns, train, test):\n",
    "    \n",
    "    ####### 稀疏特征处理部分 ##########\n",
    "    sparse_input = []\n",
    "    lr_embedding = []\n",
    "    fm_embedding = []\n",
    "    for col in sparse_columns:\n",
    "        ####### 稀疏特征转换 ##########\n",
    "        _input = Input(shape=(1,))\n",
    "        sparse_input.append(_input)\n",
    "        \n",
    "        nums = pd.concat((train[col], test[col])).nunique()\n",
    "        embed = Embedding(nums, 1, embeddings_regularizer=tf.keras.regularizers.l2(0.1))(_input)\n",
    "        embed = Flatten()(embed)\n",
    "        lr_embedding.append(embed)\n",
    "        \n",
    "        ####### fm层喂入向量初始化 ##########\n",
    "        embed = Embedding(nums, 10, embeddings_regularizer=tf.keras.regularizers.l2(0.2))(_input)\n",
    "        reshape = Reshape((10,))(embed)\n",
    "        fm_embedding.append(reshape)\n",
    "    \n",
    "    ####### fm处理层 ##########\n",
    "    fm_square = Lambda(lambda x: K.square(x))(Add()(fm_embedding))\n",
    "    square_fm = Add()([Lambda(lambda x:K.square(x))(embed)\n",
    "                     for embed in fm_embedding])\n",
    "    snd_order_sparse_layer = subtract([fm_square, square_fm])\n",
    "    snd_order_sparse_layer = Lambda(lambda x: x * 0.5)(snd_order_sparse_layer)\n",
    "    \n",
    "    ####### 数值特征处理 ##########\n",
    "    dense_input = []\n",
    "    for col in dense_columns:\n",
    "        _input = Input(shape=(1,))\n",
    "        dense_input.append(_input)\n",
    "    concat_dense_input = concatenate(dense_input)\n",
    "    fst_order_dense_layer = Activation(activation=\"relu\")(BatchNormalization()(Dense(4)(concat_dense_input)))\n",
    "    \n",
    "    ####### 线性部分拼接 ##########\n",
    "    fst_order_sparse_layer = concatenate(lr_embedding)\n",
    "    linear_part = concatenate([fst_order_dense_layer, fst_order_sparse_layer])\n",
    "    \n",
    "    ####### fm向量与数值特征拼接喂入FC部分 ##########\n",
    "    concat_fm_embedding = concatenate(fm_embedding)\n",
    "    concat_fm_embedding_dense = concatenate([concat_fm_embedding, fst_order_dense_layer])\n",
    "    fc_layer = Dropout(0.2)(Activation(activation=\"relu\")(BatchNormalization()(Dense(128)(concat_fm_embedding_dense))))\n",
    "    fc_layer = Dropout(0.2)(Activation(activation=\"relu\")(BatchNormalization()(Dense(64)(fc_layer))))\n",
    "    fc_layer = Dropout(0.2)(Activation(activation=\"relu\")(BatchNormalization()(Dense(32)(fc_layer))))\n",
    "    \n",
    "    ######## 输出层 ##########\n",
    "    output_layer = concatenate([linear_part, snd_order_sparse_layer, fc_layer])\n",
    "    output_layer = Dense(1, activation='sigmoid')(output_layer)\n",
    "    \n",
    "    model = Model(inputs=sparse_input+dense_input, outputs=output_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = deepfm_model(categorical_features, numerical_features, trn_x, val_x)\n",
    "model.compile(optimizer=\"adam\", \n",
    "              loss=\"binary_crossentropy\", \n",
    "              metrics=[\"binary_crossentropy\", tf.keras.metrics.AUC(name='auc')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sparse_x = [trn_x[f].values for f in categorical_features]\n",
    "train_dense_x = [trn_x[f].values for f in numerical_features]\n",
    "train_label = [trn_y]\n",
    "\n",
    "valid_sparse_x = [val_x[f].values for f in categorical_features]\n",
    "valid_dense_x = [val_x[f].values for f in numerical_features]\n",
    "valid_label = [val_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 82000000 samples, validate on 5000000 samples\n",
      "Epoch 1/50\n",
      "81993728/82000000 [============================>.] - ETA: 0s - loss: 0.0102 - binary_crossentropy: 0.0063 - auc: 0.9286\n",
      "Epoch 00001: val_auc improved from -inf to 0.95615, saving model to deepfm_model.h5\n",
      "82000000/82000000 [==============================] - 685s 8us/sample - loss: 0.0102 - binary_crossentropy: 0.0063 - auc: 0.9287 - val_loss: 0.0087 - val_binary_crossentropy: 0.0043 - val_auc: 0.9562\n",
      "Epoch 2/50\n",
      "81993728/82000000 [============================>.] - ETA: 0s - loss: 0.0101 - binary_crossentropy: 0.0063 - auc: 0.9287\n",
      "Epoch 00002: val_auc did not improve from 0.95615\n",
      "82000000/82000000 [==============================] - 683s 8us/sample - loss: 0.0101 - binary_crossentropy: 0.0063 - auc: 0.9287 - val_loss: 0.0083 - val_binary_crossentropy: 0.0046 - val_auc: 0.9505\n",
      "Epoch 3/50\n",
      "81993728/82000000 [============================>.] - ETA: 0s - loss: 0.0101 - binary_crossentropy: 0.0063 - auc: 0.9289\n",
      "Epoch 00003: val_auc improved from 0.95615 to 0.95988, saving model to deepfm_model.h5\n",
      "82000000/82000000 [==============================] - 689s 8us/sample - loss: 0.0101 - binary_crossentropy: 0.0063 - auc: 0.9289 - val_loss: 0.0108 - val_binary_crossentropy: 0.0071 - val_auc: 0.9599\n",
      "Epoch 4/50\n",
      "80568320/82000000 [============================>.] - ETA: 11s - loss: 0.0102 - binary_crossentropy: 0.0063 - auc: 0.9286"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60751872/82000000 [=====================>........] - ETA: 2:52 - loss: 0.0102 - binary_crossentropy: 0.0063 - auc: 0.9288"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32530432/82000000 [==========>...................] - ETA: 6:42 - loss: 0.0101 - binary_crossentropy: 0.0063 - auc: 0.9285"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import *\n",
    "# 回调函数\n",
    "filepath = \"deepfm_model.h5\"\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath, monitor='val_auc', verbose=1, save_best_only=True, mode='max', save_weights_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_auc', factor=0.5, patience=3, min_lr=0.0001, verbose=1)\n",
    "earlystopping = EarlyStopping(\n",
    "    monitor='val_auc', min_delta=0.0001, patience=5, verbose=1, mode='max')\n",
    "\n",
    "callbacks = [checkpoint, earlystopping]\n",
    "\n",
    "hist = model.fit(train_sparse_x+train_dense_x, \n",
    "                  train_label,\n",
    "                  batch_size=8192,\n",
    "                  epochs=50,\n",
    "                  validation_data=(valid_sparse_x+valid_dense_x, valid_label),\n",
    "                  callbacks=callbacks,\n",
    "                  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sparse_x = [test_x[f].values for f in categorical_features]\n",
    "test_dense_x = [test_x[f].values for f in numerical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test_sparse_x+test_dense_x, batch_size=4096, verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
