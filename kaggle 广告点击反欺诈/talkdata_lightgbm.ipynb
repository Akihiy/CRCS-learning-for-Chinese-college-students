{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './input/'\n",
    "\n",
    "train_columns = ['ip', 'app', 'device', 'os', 'channel', 'click_time', 'is_attributed']\n",
    "test_columns  = ['ip', 'app', 'device', 'os', 'channel', 'click_time', 'click_id']\n",
    "dtypes = {\n",
    "        'ip'            : 'uint32',\n",
    "        'app'           : 'uint16',\n",
    "        'device'        : 'uint16',\n",
    "        'os'            : 'uint16',\n",
    "        'channel'       : 'uint16',\n",
    "        'is_attributed' : 'uint8',\n",
    "        'click_id'      : 'uint32'\n",
    "        }\n",
    "\n",
    "train = pd.read_csv(path+'train.csv', usecols=train_columns, dtype=dtypes, parse_dates=['click_time'])\n",
    "test = pd.read_csv(path+'test.csv', usecols=test_columns, dtype=dtypes, parse_dates=['click_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集label\n",
    "y_train = train['is_attributed'].values\n",
    "\n",
    "sub = test[['click_id']]\n",
    "del test['click_id']\n",
    "\n",
    "# 训练集与测试集合并\n",
    "data = pd.concat([train, test], axis=0)\n",
    "del train, test\n",
    "gc.collect()\n",
    "\n",
    "# 时间处理\n",
    "data['day'] = data['click_time'].dt.day.astype('uint8')\n",
    "data['hour'] = data['click_time'].dt.hour.astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目标编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [4:44:09<00:00, 2435.68s/it]  \n"
     ]
    }
   ],
   "source": [
    "for cols in tqdm([['ip'], ['app'], ['ip','app'], ['ip','hour'], ['ip','os','device'], ['ip','app','os','device'], ['app','os','channel']]):\n",
    "    name = '_'.join(cols)\n",
    "    res = pd.DataFrame()\n",
    "    temp = data[cols + ['day', 'is_attributed']]\n",
    "    for period in [7,8,9,10]:\n",
    "        mean_ = temp[temp['day']<period].groupby(cols)['is_attributed'].mean().reset_index(name=name + '_mean_is_attributed')\n",
    "        mean_['day'] = period\n",
    "        res = res.append(mean_, ignore_index=True)\n",
    "    \n",
    "    data = pd.merge(data, res, how='left', on=['day']+cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 保留7 8 9 10号数据\n",
    "data = data[data['day']>=7]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 统计特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [04:10<00:00, 41.67s/it]\n",
      "100%|██████████| 4/4 [07:00<00:00, 105.15s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count\n",
    "for cols in tqdm([['ip'],['ip','app'],['ip','os','device'],['ip','day','hour'],\\\n",
    "                  ['app','channel','day','hour'],['ip','device','day','hour']]):\n",
    "    name = '_'.join(cols)\n",
    "    data[name+'_cnts'] = data.groupby(cols)['click_time'].transform('count')\n",
    "    data[name+'_cnts'] = data[name+'_cnts'].astype('uint16')\n",
    "\n",
    "# nunique\n",
    "for f1 in ['ip']:\n",
    "    for f2 in tqdm(['app','device','os','channel']):\n",
    "        data[f1+'_'+f2+'_nuni'] = data.groupby([f1])[f2].transform('nunique')\n",
    "        data[f1+'_'+f2+'_nuni'] = data[f1+'_'+f2+'_nuni'].astype('uint16') \n",
    "            \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 时间差特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [2:00:50<00:00, 3625.38s/it]\n"
     ]
    }
   ],
   "source": [
    "for cols in tqdm([['ip','os','device','app'],['ip','os','device','app','day']]):\n",
    "    for i in range(1,6):\n",
    "        \n",
    "        data['ct'] = (data['click_time'].astype(np.int64)//10**9).astype(np.int32)\n",
    "        \n",
    "        name = '{}_next_{}_click'.format('_'.join(cols), str(i))\n",
    "        data[name] = (data.groupby(cols).ct.shift(-i)-data.ct).astype(np.float32)\n",
    "        data[name] = data[name].fillna(data[name].mean())\n",
    "        data[name] = data[name].astype('uint16')\n",
    "        \n",
    "        name = '{}_lag_{}_click'.format('_'.join(cols), str(i))\n",
    "        data[name] = (data.groupby(cols).ct.shift(i)-data.ct).astype(np.float32)\n",
    "        data[name] = data[name].fillna(data[name].mean())\n",
    "        data[name] = data[name].astype('uint16')\n",
    "        \n",
    "        data.drop(['ct'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = ['ip', 'os', 'device', 'app']\n",
    "data['click_user_lab'] = 0\n",
    "pos = data.duplicated(subset=subset, keep=False)\n",
    "data.loc[pos, 'click_user_lab'] = 1\n",
    "pos = (~data.duplicated(subset=subset, keep='first')) & data.duplicated(subset=subset, keep=False)\n",
    "data.loc[pos, 'click_user_lab'] = 2\n",
    "pos = (~data.duplicated(subset=subset, keep='last')) & data.duplicated(subset=subset, keep=False)\n",
    "data.loc[pos, 'click_user_lab'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 排序特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [12:40<00:00, 380.28s/it]\n"
     ]
    }
   ],
   "source": [
    "for cols in tqdm([['ip','os','device','app'],['ip','os','device','app','day']]):\n",
    "    name = '{}_click_asc_rank'.format('_'.join(cols)) \n",
    "    data[name] = data.groupby(cols)['click_time'].rank(ascending=True)\n",
    "    \n",
    "    name = '{}_click_dec_rank'.format('_'.join(cols)) \n",
    "    data[name] = data.groupby(cols)['click_time'].rank(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练集/验证集/测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ip',\n",
       " 'app',\n",
       " 'device',\n",
       " 'os',\n",
       " 'channel',\n",
       " 'day',\n",
       " 'hour',\n",
       " 'ip_mean_is_attributed',\n",
       " 'app_mean_is_attributed',\n",
       " 'ip_app_mean_is_attributed',\n",
       " 'ip_hour_mean_is_attributed',\n",
       " 'ip_os_device_mean_is_attributed',\n",
       " 'ip_app_os_device_mean_is_attributed',\n",
       " 'app_os_channel_mean_is_attributed',\n",
       " 'ip_cnts',\n",
       " 'ip_app_cnts',\n",
       " 'ip_os_device_cnts',\n",
       " 'ip_day_hour_cnts',\n",
       " 'app_channel_day_hour_cnts',\n",
       " 'ip_device_day_hour_cnts',\n",
       " 'ip_app_nuni',\n",
       " 'ip_device_nuni',\n",
       " 'ip_os_nuni',\n",
       " 'ip_channel_nuni',\n",
       " 'ip_os_device_app_next_1_click',\n",
       " 'ip_os_device_app_lag_1_click',\n",
       " 'ip_os_device_app_next_2_click',\n",
       " 'ip_os_device_app_lag_2_click',\n",
       " 'ip_os_device_app_next_3_click',\n",
       " 'ip_os_device_app_lag_3_click',\n",
       " 'ip_os_device_app_next_4_click',\n",
       " 'ip_os_device_app_lag_4_click',\n",
       " 'ip_os_device_app_next_5_click',\n",
       " 'ip_os_device_app_lag_5_click',\n",
       " 'ip_os_device_app_day_next_1_click',\n",
       " 'ip_os_device_app_day_lag_1_click',\n",
       " 'ip_os_device_app_day_next_2_click',\n",
       " 'ip_os_device_app_day_lag_2_click',\n",
       " 'ip_os_device_app_day_next_3_click',\n",
       " 'ip_os_device_app_day_lag_3_click',\n",
       " 'ip_os_device_app_day_next_4_click',\n",
       " 'ip_os_device_app_day_lag_4_click',\n",
       " 'ip_os_device_app_day_next_5_click',\n",
       " 'ip_os_device_app_day_lag_5_click',\n",
       " 'click_user_lab',\n",
       " 'ip_os_device_app_click_asc_rank',\n",
       " 'ip_os_device_app_click_dec_rank',\n",
       " 'ip_os_device_app_day_click_asc_rank',\n",
       " 'ip_os_device_app_day_click_dec_rank']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features = ['ip','app','os','channel','device','day','hour']\n",
    "features = [f for f in data.columns if f not in ['click_time','is_attributed']]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 21318.78 Mb (40.7% reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = reduce_mem_usage(data)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 负采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 对训练集进行负采样\n",
    "# df_train_neg = data[(data['is_attributed'] == 0)&(data['day'] < 9)]\n",
    "# df_train_neg = df_train_neg.sample(n=1000000)\n",
    "\n",
    "# # 合并成新的数据集\n",
    "# df_rest = data[(data['is_attributed'] == 1)|(data['day'] >= 9)]\n",
    "# data = pd.concat([df_train_neg, df_rest]).sample(frac=1)\n",
    "# del df_train_neg\n",
    "# del df_rest\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_x = data[data['day']<9][features]\n",
    "trn_y = data[data['day']<9]['is_attributed']\n",
    "\n",
    "val_x = data[data['day']==9][features]\n",
    "val_y = data[data['day']==9]['is_attributed']\n",
    "\n",
    "test_x = data[data['day']>9][features]\n",
    "\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1040: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:685: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's auc: 0.913983\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[2]\tvalid_0's auc: 0.555097\n",
      "[3]\tvalid_0's auc: 0.715026\n",
      "[4]\tvalid_0's auc: 0.724532\n",
      "[5]\tvalid_0's auc: 0.759976\n",
      "[6]\tvalid_0's auc: 0.76629\n",
      "[7]\tvalid_0's auc: 0.796175\n",
      "[8]\tvalid_0's auc: 0.833505\n",
      "[9]\tvalid_0's auc: 0.84212\n",
      "[10]\tvalid_0's auc: 0.843283\n",
      "[11]\tvalid_0's auc: 0.850733\n",
      "[12]\tvalid_0's auc: 0.871978\n",
      "[13]\tvalid_0's auc: 0.875328\n",
      "[14]\tvalid_0's auc: 0.875165\n",
      "[15]\tvalid_0's auc: 0.87586\n",
      "[16]\tvalid_0's auc: 0.880049\n",
      "[17]\tvalid_0's auc: 0.88159\n",
      "[18]\tvalid_0's auc: 0.883265\n",
      "[19]\tvalid_0's auc: 0.886364\n",
      "[20]\tvalid_0's auc: 0.884776\n",
      "[21]\tvalid_0's auc: 0.886904\n",
      "[22]\tvalid_0's auc: 0.890412\n",
      "[23]\tvalid_0's auc: 0.888791\n",
      "[24]\tvalid_0's auc: 0.89301\n",
      "[25]\tvalid_0's auc: 0.894779\n",
      "[26]\tvalid_0's auc: 0.894971\n",
      "[27]\tvalid_0's auc: 0.895712\n",
      "[28]\tvalid_0's auc: 0.895321\n",
      "[29]\tvalid_0's auc: 0.8961\n",
      "[30]\tvalid_0's auc: 0.896595\n",
      "[31]\tvalid_0's auc: 0.900489\n",
      "[32]\tvalid_0's auc: 0.900557\n",
      "[33]\tvalid_0's auc: 0.901255\n",
      "[34]\tvalid_0's auc: 0.901816\n",
      "[35]\tvalid_0's auc: 0.903473\n",
      "[36]\tvalid_0's auc: 0.904417\n",
      "[37]\tvalid_0's auc: 0.904424\n",
      "[38]\tvalid_0's auc: 0.904806\n",
      "[39]\tvalid_0's auc: 0.90534\n",
      "[40]\tvalid_0's auc: 0.907087\n",
      "[41]\tvalid_0's auc: 0.907099\n",
      "[42]\tvalid_0's auc: 0.907347\n",
      "[43]\tvalid_0's auc: 0.90937\n",
      "[44]\tvalid_0's auc: 0.909649\n",
      "[45]\tvalid_0's auc: 0.90945\n",
      "[46]\tvalid_0's auc: 0.908987\n",
      "[47]\tvalid_0's auc: 0.908216\n",
      "[48]\tvalid_0's auc: 0.909112\n",
      "[49]\tvalid_0's auc: 0.909138\n",
      "[50]\tvalid_0's auc: 0.909197\n",
      "[51]\tvalid_0's auc: 0.909502\n",
      "[52]\tvalid_0's auc: 0.908868\n",
      "[53]\tvalid_0's auc: 0.909191\n",
      "[54]\tvalid_0's auc: 0.908427\n",
      "[55]\tvalid_0's auc: 0.908496\n",
      "[56]\tvalid_0's auc: 0.908194\n",
      "[57]\tvalid_0's auc: 0.908041\n",
      "[58]\tvalid_0's auc: 0.907421\n",
      "[59]\tvalid_0's auc: 0.908215\n",
      "[60]\tvalid_0's auc: 0.90791\n",
      "[61]\tvalid_0's auc: 0.907886\n",
      "[62]\tvalid_0's auc: 0.908252\n",
      "[63]\tvalid_0's auc: 0.908215\n",
      "[64]\tvalid_0's auc: 0.908312\n",
      "[65]\tvalid_0's auc: 0.908128\n",
      "[66]\tvalid_0's auc: 0.908056\n",
      "[67]\tvalid_0's auc: 0.907493\n",
      "[68]\tvalid_0's auc: 0.9074\n",
      "[69]\tvalid_0's auc: 0.907475\n",
      "[70]\tvalid_0's auc: 0.907559\n",
      "[71]\tvalid_0's auc: 0.90778\n",
      "[72]\tvalid_0's auc: 0.907797\n",
      "[73]\tvalid_0's auc: 0.908183\n",
      "[74]\tvalid_0's auc: 0.907688\n",
      "[75]\tvalid_0's auc: 0.908556\n",
      "[76]\tvalid_0's auc: 0.907833\n",
      "[77]\tvalid_0's auc: 0.908018\n",
      "[78]\tvalid_0's auc: 0.908011\n",
      "[79]\tvalid_0's auc: 0.907976\n",
      "[80]\tvalid_0's auc: 0.906898\n",
      "[81]\tvalid_0's auc: 0.907684\n",
      "[82]\tvalid_0's auc: 0.907691\n",
      "[83]\tvalid_0's auc: 0.907608\n",
      "[84]\tvalid_0's auc: 0.908389\n",
      "[85]\tvalid_0's auc: 0.908401\n",
      "[86]\tvalid_0's auc: 0.907614\n",
      "[87]\tvalid_0's auc: 0.908031\n",
      "[88]\tvalid_0's auc: 0.908216\n",
      "[89]\tvalid_0's auc: 0.90267\n",
      "[90]\tvalid_0's auc: 0.902572\n",
      "[91]\tvalid_0's auc: 0.90244\n",
      "[92]\tvalid_0's auc: 0.902577\n",
      "[93]\tvalid_0's auc: 0.90268\n",
      "[94]\tvalid_0's auc: 0.903147\n",
      "[95]\tvalid_0's auc: 0.903485\n",
      "[96]\tvalid_0's auc: 0.90322\n",
      "[97]\tvalid_0's auc: 0.903603\n",
      "[98]\tvalid_0's auc: 0.904215\n",
      "[99]\tvalid_0's auc: 0.900565\n",
      "[100]\tvalid_0's auc: 0.903906\n",
      "[101]\tvalid_0's auc: 0.902498\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.913983\n"
     ]
    }
   ],
   "source": [
    "params = {'num_leaves': 127,\n",
    "          'min_data_in_leaf': 32, \n",
    "          'objective':'binary',\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.1,\n",
    "          'min_child_samples': 20,\n",
    "          'boosting': 'gbdt',\n",
    "          'feature_fraction': 0.8,\n",
    "          'bagging_freq': 1,\n",
    "          'bagging_fraction': 0.8 ,\n",
    "          'bagging_seed': 11,\n",
    "          'metric': 'auc',\n",
    "          'lambda_l1': 0.1,\n",
    "          'verbosity': -1\n",
    "         }\n",
    "train_data = lgb.Dataset(trn_x.values.astype(np.float32), label=trn_y,\n",
    "                         categorical_feature=categorical_features, feature_name=features)\n",
    "valid_data = lgb.Dataset(val_x.values.astype(np.float32), label=val_y,\n",
    "                         categorical_feature=categorical_features, feature_name=features)\n",
    "\n",
    "clf = lgb.train(params,\n",
    "                train_data,\n",
    "                10000,\n",
    "                early_stopping_rounds=100,\n",
    "                valid_sets=[valid_data],\n",
    "                verbose_eval=1\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'eta': 0.2,\n",
    "          'max_leaves': 2**9-1,  \n",
    "          'max_depth': 9, \n",
    "          'subsample': 0.7, \n",
    "          'colsample_bytree': 0.9, \n",
    "          'objective': 'binary:logistic', \n",
    "          'scale_pos_weight':9,\n",
    "          'eval_metric': 'auc', \n",
    "          'n_jobs':24,\n",
    "          'random_state': 2020,\n",
    "          'silent': True}\n",
    "          \n",
    "dtrain = xgb.DMatrix(trn_x, trn_y)\n",
    "dvalid = xgb.DMatrix(val_x, val_y)\n",
    "gc.collect()\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "model = xgb.train(params, dtrain, 200, watchlist, early_stopping_rounds = 20, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练集合并验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_x = pd.concat([trn_x, val_x], axis=0, ignore_index=True)\n",
    "trn_y = np.r_[trn_y, val_y] # 是按列连接两个矩阵，就是把两矩阵上下相加，要求列数相等，类似于pandas中的concat() \n",
    "del val_x\n",
    "del val_y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(trn_x.values.astype(np.float32), label=trn_y,\n",
    "                        categorical_feature=categorical_features, feature_name=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = 400\n",
    "clf = lgb.train(params,\n",
    "                train_data,\n",
    "                int(trees * 1.2),\n",
    "                valid_sets=[train_data],\n",
    "                verbose_eval=10\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
